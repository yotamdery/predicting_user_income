{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51baa487-f643-45aa-a09a-51bfb6a4eca7",
   "metadata": {},
   "source": [
    "# Author: Yotam Dery\n",
    "# Date: 03/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc8435",
   "metadata": {},
   "source": [
    "# Part 1 - Prediction task - user income prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd0c43-e464-4c05-a28d-3945237808e7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f083ae2-0b14-4f52-b473-0de4a8b50664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "# ML related\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from scipy.stats import skew\n",
    "# Projects scripts\n",
    "from prediction_task_plot_utils import plot_target_label, plot_features_dist, plot_corr_with_label\n",
    "from prediction_task_utils import check_skewness_score, apply_log_transformation, \\\n",
    "                                    identify_skewed_features, identify_highly_correlated_features,\\\n",
    "                                    get_outliers_top_n_features\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588201f-ab80-438d-9e5c-de22b7a35f6e",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169beeb2-5632-48f8-87d9-9733ef927a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train data\n",
    "df_train = pd.read_csv('train_home_assignment.csv', index_col=0)\n",
    "# Dropping columns for this task\n",
    "df_train = df_train.drop(columns=[\"treatment\", \"org_price_usd_following_30_days_after_impact\"])\n",
    "print('train shape is: {}'.format(df_train.shape))\n",
    "\n",
    "# Loading the test data\n",
    "df_test = pd.read_csv('test_home_assignment.csv', index_col=0)\n",
    "print('test shape is: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094531b-423f-4a9a-9d72-154be5b497b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First inspect of the train set\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ef3a0-4e97-4ff1-92b2-b6034235c65e",
   "metadata": {},
   "source": [
    "# Train-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee33a4c-8db3-4888-99ca-c5fe9b04f562",
   "metadata": {},
   "source": [
    "* We'd like to first perfrom the train-validation split to ensure that the needed operations are made using training set statistics, <br>\n",
    "and to prevent data leakage. <br>\n",
    "We'll use a 80-20 Split (80% for training, 20% for validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552f6c9-bcf2-41da-ba3b-b257d68569e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_train.drop(columns=[\"org_price_usd_following_30_days\"])\n",
    "y = df_train[\"org_price_usd_following_30_days\"]\n",
    "\n",
    "# Create separate targets:\n",
    "y_log = np.log1p(y)  # Log transformation (only for Linear Regression)\n",
    "y_original = y  # Keep original target for tree-based models\n",
    "\n",
    "# Split into training & validation sets\n",
    "X_train, X_val, y_train_log, y_val_log = train_test_split(X, y_log, test_size=0.2, random_state=42)  # Log target for linear\n",
    "_, _, y_train_original, y_val_original = train_test_split(X, y_original, test_size=0.2, random_state=42)  # Original target for trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aaa274-8d96-42f7-8473-bba715347435",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f13989-d460-496f-baf4-9f6677a9ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print some statistics for the train set\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic structure of the dataset\n",
    "train_info = X_train.info()\n",
    "train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_info = X_val.info()\n",
    "validation_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539be0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = X_train.isnull().sum().sort_values(ascending=False)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae46a13",
   "metadata": {},
   "source": [
    "* <b>Insights from previous steps:</b>\n",
    "1. Training set: 160,000 rows\n",
    "Validation set: 40,000 rows \n",
    "\n",
    "2. Dataset Structure\n",
    "53 features (excluding the target variable). <br>\n",
    "All features are numerical (float64 or int64). <br>\n",
    "There are no missing values. <br>\n",
    "Some features have high variance, indicating possible outliers. <br>\n",
    "Certain features have long tails, meaning a log transformation may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b663e-c61a-4ee0-9aea-71a710a97964",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the target label histogram\n",
    "# Define fixed bin edges from 0 to 800 with intervals of 50\n",
    "start, end, step = 0, 850, 50\n",
    "bin_edges = np.arange(start, end, step)  # 850 ensures last bin covers up to 800\n",
    "plot_target_label(bin_edges, y_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c722a",
   "metadata": {},
   "source": [
    "* The target label looks very skewed (strong right tail). <br> If we consider models that assume normally distributed residuals (like linear regression), we might want to perform a log transformation to the target label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0806a2",
   "metadata": {},
   "source": [
    "* <b> Skewness criteria: </b> <br>\n",
    "Skewness > 1.0 → highly skewed (log transform might help). <br>\n",
    "Skewness between 0.5 and 1.0 → moderately skewed (consider transforming). <br>\n",
    "Skewness < 0.5 → nearly symmetric (transformation unnecessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28509132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_skewness_score(y_train_original))\n",
    "print(\"Looks like the target label is highly skewed before the trasformation and symmetric after!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98aa57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transformed target label\n",
    "y_train_log_transformed = apply_log_transformation(y_train_original)\n",
    "start, end, step = 0, 11, 1\n",
    "bin_edges = np.arange(start, end, step)  \n",
    "plot_target_label(bin_edges, y_train_log_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e5b93-d6d2-427e-b99a-6baeb4c3227e",
   "metadata": {},
   "source": [
    "## Univariate Analysis (Features Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa8af7-223a-4769-a62b-40ea0e43fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_dist(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d49420",
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_skewed_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For a deeper profiling (but takes long time for execution)\n",
    "# profile = ProfileReport(X_train, explorative=True)\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9501f1-640a-40b7-a73a-c3c014d5f330",
   "metadata": {},
   "source": [
    "Insights from Univariate Analysis: <br>\n",
    "1. Many real-world use cases (like user spending, income, and engagement metrics) follow a right-skewed distribution (like we see here!).<br> <br>\n",
    "2. Some features (e.g., payment_occurrences_preceding_30_days,payment_occurrences_preceding_3_days) have right-skewed distributions, meaning log transformation might help. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655c901-8b37-427b-bc84-8360b71a297b",
   "metadata": {},
   "source": [
    "## Bivariate Analysis (Feature Relationships)\n",
    "In this step, we'll analyze how different features relate to the target variable (tag) to uncover important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236755ba-dea0-4e0f-8155-b46d9682ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_with_label(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for highly correlated pairs of features\n",
    "identify_highly_correlated_features(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab319924",
   "metadata": {},
   "source": [
    "Here are all pairs of highly correlated features (corr>0.9).<br> Some highly correlated features might capture different patterns (e.g., spending behavior vs. engagement behavior), <br>so we wont drop them for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7752cb0",
   "metadata": {},
   "source": [
    "## Outliers Detection\n",
    "* Here we perform the outliers detection for the top 20 highly correlated features (with respect to the target label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0ab40-b6d2-4ca6-bf76-9179e21f226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outliers_top_n_features(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4e1f8-a38e-4089-b367-1a01ac2ed567",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1590f87-2649-4689-b4a8-3c541fdca2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer to preprocess data for machine learning models.\n",
    "    \n",
    "    - For Linear Regression:\n",
    "        - Applies log transformation (with automatic negative value shifting).\n",
    "        - Removes outliers using IQR clipping.\n",
    "        - Standardizes features using StandardScaler.\n",
    "        - Optionally removes highly correlated features.\n",
    "    \n",
    "    - For Tree-Based Models (Random Forest, XGBoost):\n",
    "        - Skips outlier removal (trees handle outliers naturally).\n",
    "        - Skips scaling (trees do not require feature scaling).\n",
    "        - Optionally removes highly correlated features.\n",
    "\n",
    "    Parameters:\n",
    "        model_type (str): \"linear\" for Linear Regression, \"tree\" for RF/XGBoost.\n",
    "        remove_high_corr_features (bool): If True, removes highly correlated features.\n",
    "\n",
    "    Methods:\n",
    "        fit(X, y): Learns dataset statistics.\n",
    "        transform(X): Applies transformations using learned statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_type=\"linear\", remove_high_corr_features=False):\n",
    "        self.model_type = model_type\n",
    "        self.remove_high_corr_features = remove_high_corr_features\n",
    "        self.log_features = []\n",
    "        self.scaler = None\n",
    "        self.iqr_limits = {}\n",
    "        self.high_corr_features = {}\n",
    "        self.shift_values = {}  # Stores shift values for negative log-transformed features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learns dataset statistics for transformations.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Training data.\n",
    "            y (pd.Series, optional): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        # Identify skewed features for log transformation (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            skewness = X.apply(lambda x: skew(x), axis=0)\n",
    "            self.log_features = skewness[abs(skewness) > 1].index.tolist()\n",
    "\n",
    "            # Detect and store shift values for negative log-transformed features\n",
    "            for feature in self.log_features:\n",
    "                min_value = X[feature].min()\n",
    "                if min_value <= 0:\n",
    "                    self.shift_values[feature] = abs(min_value) + 1  # Shift to make values positive\n",
    "\n",
    "        # Compute IQR thresholds for outlier handling (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            for feature in X.columns:\n",
    "                Q1 = X[feature].quantile(0.25)\n",
    "                Q3 = X[feature].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                self.iqr_limits[feature] = (lower_bound, upper_bound)\n",
    "\n",
    "        # Compute feature scaling using StandardScaler (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(X)\n",
    "\n",
    "        # Identify highly correlated features for removal (if enabled)\n",
    "        if self.remove_high_corr_features:\n",
    "            corr_matrix = X.corr()\n",
    "            high_corr_pairs = set()\n",
    "            for col in corr_matrix.columns:\n",
    "                for idx in corr_matrix.index:\n",
    "                    if col != idx and abs(corr_matrix.loc[idx, col]) > 0.9:\n",
    "                        high_corr_pairs.add((col, idx))\n",
    "            self.high_corr_features = list(set([pair[1] for pair in high_corr_pairs]))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies transformations to the dataset using the learned statistics.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Dataset to transform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed dataset.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Apply log transformation (only for Linear Regression)\n",
    "        if self.model_type == \"linear\" and self.log_features:\n",
    "            for feature in self.log_features:\n",
    "                # Apply the shift if required (to prevent log of negative numbers)\n",
    "                if feature in self.shift_values:\n",
    "                    X_transformed[feature] += self.shift_values[feature]\n",
    "                X_transformed[feature] = np.log1p(X_transformed[feature])\n",
    "\n",
    "        # Apply outlier handling using IQR clipping (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            for feature, (lower_bound, upper_bound) in self.iqr_limits.items():\n",
    "                X_transformed[feature] = np.clip(X_transformed[feature], lower_bound, upper_bound)\n",
    "\n",
    "        # Apply feature scaling using StandardScaler (only for Linear Regression)\n",
    "        if self.model_type == \"linear\" and self.scaler:\n",
    "            X_transformed = pd.DataFrame(self.scaler.transform(X_transformed), columns=X_transformed.columns)\n",
    "\n",
    "        # Drop highly correlated features (if enabled)\n",
    "        if self.remove_high_corr_features and self.high_corr_features:\n",
    "            X_transformed = X_transformed.drop(columns=self.high_corr_features, errors=\"ignore\")\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7fcfd7-0c40-469c-9d50-585c2ee84e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transformer for Linear Regression (without correlated feature removal)\n",
    "preprocessor_linear = CustomPreprocessor(model_type=\"linear\", remove_high_corr_features=False)\n",
    "\n",
    "# Fit and transform for Linear Regression\n",
    "preprocessor_linear.fit(X_train)\n",
    "X_train_transformed_linear = preprocessor_linear.transform(X_train)\n",
    "X_val_transformed_linear = preprocessor_linear.transform(X_val)\n",
    "# Output transformed training data sample (Linear Model)\n",
    "X_train_transformed_linear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a764d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transformer for Tree-Based Models (without correlated feature removal)\n",
    "preprocessor_tree = CustomPreprocessor(model_type=\"tree\", remove_high_corr_features=False)\n",
    "\n",
    "# Fit and transform for Tree-Based Models\n",
    "preprocessor_tree.fit(X_train)\n",
    "X_train_transformed_tree = preprocessor_tree.transform(X_train)\n",
    "X_val_transformed_tree = preprocessor_tree.transform(X_val)\n",
    "\n",
    "# Output transformed training data sample (Tree-Based Model)\n",
    "X_train_transformed_tree.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ef910-9554-4da9-9242-af14513b2455",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5775d-79df-45c7-8ada-74b3d889d6f6",
   "metadata": {},
   "source": [
    "## Create a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d313e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check for potential RMSE - without a model\n",
    "np.sqrt(mean_squared_error(y_val_original, X_val['org_price_usd_preceding_30_days']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model using the preprocessed training data (X_train) and log-transformed target (y_train_log)\n",
    "linear_model.fit(X_train_transformed_linear, y_train_log)\n",
    "\n",
    "# Predict on the validation set (output is still in log scale)\n",
    "y_pred_log = linear_model.predict(X_val_transformed_linear)\n",
    "\n",
    "# Convert predictions back to original scale using expm1()\n",
    "y_pred_original = np.expm1(y_pred_log)  # Reverse log1p transformation\n",
    "\n",
    "# Compute RMSE (Root Mean Squared Error) in the original scale\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_val_original, y_pred_original))\n",
    "\n",
    "# Output RMSE\n",
    "rmse_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9cead",
   "metadata": {},
   "source": [
    "## Create and tune the tree based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c1f1b-cc1b-4332-b2ef-7064a001e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200],  # Number of trees\n",
    "    \"max_depth\": [10, 20],  # Depth of trees\n",
    "}\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42, criterion=\"squared_error\")\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_model, rf_param_grid, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, verbose=1\n",
    ")\n",
    "rf_grid_search.fit(X_train_transformed_tree, y_train_original)\n",
    "\n",
    "# Get best model & best parameters\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "best_rf_params = rf_grid_search.best_params_\n",
    "\n",
    "# Predict using best model\n",
    "y_pred_rf_best = best_rf_model.predict(X_val_transformed_tree)\n",
    "\n",
    "# Compute RMSE for best RF model\n",
    "rmse_rf_best = np.sqrt(mean_squared_error(y_val_original, y_pred_rf_best))\n",
    "\n",
    "# Output best parameters and RMSE\n",
    "best_rf_params, rmse_rf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 150, 200],  # Number of boosting rounds\n",
    "    \"max_depth\": [4, 6, 10],  # Depth of trees\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],  # Step size for each tree\n",
    "}\n",
    "\n",
    "# Initialize XGBoost\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\", random_state=77)\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    xgb_model, xgb_param_grid, cv=3, scoring=\"neg_root_mean_squared_error\", verbose=1\n",
    ")\n",
    "xgb_grid_search.fit(X_train_transformed_tree, y_train_original)\n",
    "\n",
    "# Get best model & best parameters\n",
    "best_xgb_model = xgb_grid_search.best_estimator_\n",
    "best_xgb_params = xgb_grid_search.best_params_\n",
    "\n",
    "# Predict using best XGBoost model\n",
    "y_pred_xgb_best = best_xgb_model.predict(X_val_transformed_tree)\n",
    "\n",
    "# Compute RMSE for best XGBoost model\n",
    "rmse_xgb_best = np.sqrt(mean_squared_error(y_val_original, y_pred_xgb_best))\n",
    "\n",
    "# Output best parameters and RMSE\n",
    "best_xgb_params, rmse_xgb_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deee99-5911-4c51-8e16-08e9a0a182a5",
   "metadata": {},
   "source": [
    "# Final prediction\n",
    "We can see that the RF model is the best model. We'll choose it to generate our final predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e58dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best RF model with optimal hyperparameters from GridSearchCV\n",
    "best_rf_model = RandomForestRegressor(\n",
    "    max_depth=best_rf_params[\"max_depth\"],\n",
    "    n_estimators=best_rf_params[\"n_estimators\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the final RF model on the full dataset\n",
    "best_rf_model.fit(X_train_transformed_tree, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90299594-4153-4cb6-aea7-dd01063e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing steps, use the trained transformer\n",
    "X_test_transformed = preprocessor_tree.transform(df_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffa5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "test_predictions = best_rf_model.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf68440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV file\n",
    "submission = pd.DataFrame({\"id\": df_test.index, \"predicted_revenue\": test_predictions})\n",
    "submission.to_csv(\"final_rf_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51133aba",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "* Here I answer the question - What are the three most important features that contributed to the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the trained RF model\n",
    "feature_importance = best_rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature names and importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X_train_transformed_tree.columns,\n",
    "    \"Importance\": feature_importance\n",
    "})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the top 3 most important features\n",
    "top_3_features = feature_importance_df.head(3)['Feature']\n",
    "print(\"The three most important features that contributed to the prediction are:\")\n",
    "list(top_3_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f027319",
   "metadata": {},
   "source": [
    "# Part 2 - Recommendation task\n",
    "For this task, I'll use T-Learner (Two-Model Approach) <br>\n",
    "How It Works: <br>\n",
    "Two separate machine learning models are trained:<br>\n",
    "Model_A: Predicts revenue if Treatment A is given.<br>\n",
    "Model_B: Predicts revenue if Treatment B is given.<br>\n",
    "The treatment effect is estimated by comparing the predictions from the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train data\n",
    "train_df = pd.read_csv('train_home_assignment.csv', index_col=0)\n",
    "\n",
    "# Loading the test data\n",
    "test_df = pd.read_csv('test_home_assignment.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e25465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = train_df.drop(columns=[\"org_price_usd_following_30_days_after_impact\", \"org_price_usd_following_30_days\"])\n",
    "y = train_df[\"org_price_usd_following_30_days_after_impact\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850535b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training & validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate datasets based on treatment\n",
    "X_train_A = X_train[X_train[\"treatment\"] == 2].drop(columns=[\"treatment\"])\n",
    "y_train_A = y_train[X_train[\"treatment\"] == 2]\n",
    "X_val_A = X_val[X_val[\"treatment\"] == 2].drop(columns=[\"treatment\"])\n",
    "y_val_A = y_val[X_val[\"treatment\"] == 2]\n",
    "\n",
    "X_train_B = X_train[X_train[\"treatment\"] == 10].drop(columns=[\"treatment\"])\n",
    "y_train_B = y_train[X_train[\"treatment\"] == 10]\n",
    "X_val_B = X_val[X_val[\"treatment\"] == 10].drop(columns=[\"treatment\"])\n",
    "y_val_B = y_val[X_val[\"treatment\"] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train separate models for Treatment A and B (Using the best parameters from the GridSearch above)\n",
    "model_A = RandomForestRegressor(\n",
    "    max_depth=best_rf_params[\"max_depth\"],\n",
    "    n_estimators=best_rf_params[\"n_estimators\"],\n",
    "    random_state=42\n",
    ")\n",
    "model_B = RandomForestRegressor(\n",
    "    max_depth=best_rf_params[\"max_depth\"],\n",
    "    n_estimators=best_rf_params[\"n_estimators\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model on its respective treatment group\n",
    "model_A.fit(X_train_A, y_train_A)\n",
    "model_B.fit(X_train_B, y_train_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using best model\n",
    "y_pred_A = model_A.predict(X_val_A)\n",
    "\n",
    "# Compute RMSE for best RF model\n",
    "rmse_rf_best = np.sqrt(mean_squared_error(y_val_A, y_pred_A))\n",
    "\n",
    "# Output RMSE\n",
    "rmse_rf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using best model\n",
    "y_pred_B = model_B.predict(X_val_B)\n",
    "\n",
    "# Compute RMSE for best RF model\n",
    "rmse_rf_best = np.sqrt(mean_squared_error(y_val_B, y_pred_B))\n",
    "\n",
    "# Output RMSE\n",
    "rmse_rf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bbfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data (remove treatment column for prediction)\n",
    "X_test = test_df.drop(columns=[\"treatment\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab06530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two copies of test data (one for each treatment)\n",
    "X_test_A = X_test.copy()\n",
    "X_test_B = X_test.copy()\n",
    "\n",
    "# Predict revenue under each treatment\n",
    "pred_A = model_A.predict(X_test_A)\n",
    "pred_B = model_B.predict(X_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the best treatment based on highest predicted revenue - in a new column\n",
    "test_df[\"optimal_treatment\"] = np.where(pred_A > pred_B, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "test_df.to_csv(\"recommended_treatments_t_learner.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from the trained treatment prediction models\n",
    "feature_importance_A = model_A.feature_importances_\n",
    "feature_importance_B = model_B.feature_importances_\n",
    "\n",
    "# Create DataFrames to store feature importance scores for each treatment model\n",
    "feature_importance_A_df = pd.DataFrame({\n",
    "    \"Feature\": X_train_A.columns,\n",
    "    \"Importance_A\": feature_importance_A\n",
    "}).sort_values(by=\"Importance_A\", ascending=False)\n",
    "\n",
    "feature_importance_B_df = pd.DataFrame({\n",
    "    \"Feature\": X_train_B.columns,\n",
    "    \"Importance_B\": feature_importance_B\n",
    "}).sort_values(by=\"Importance_B\", ascending=False)\n",
    "\n",
    "# Merge both feature importance rankings\n",
    "feature_importance_combined = feature_importance_A_df.merge(\n",
    "    feature_importance_B_df, on=\"Feature\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Compute average importance score across both models\n",
    "feature_importance_combined[\"Avg_Importance\"] = (\n",
    "    feature_importance_combined[\"Importance_A\"] + feature_importance_combined[\"Importance_B\"]\n",
    ") / 2\n",
    "\n",
    "# Sort by average importance to get the most influential features\n",
    "top_3_treatment_features = feature_importance_combined.sort_values(\n",
    "    by=\"Avg_Importance\", ascending=False\n",
    ").head(3)\n",
    "\n",
    "# Output the top 3 most important features influencing treatment decisions\n",
    "top_3_treatment_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19742c",
   "metadata": {},
   "source": [
    "# THE END :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
