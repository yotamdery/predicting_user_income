{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51baa487-f643-45aa-a09a-51bfb6a4eca7",
   "metadata": {},
   "source": [
    "# User income prediction\n",
    "## Author: Yotam Dery\n",
    "## Date: 03/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd0c43-e464-4c05-a28d-3945237808e7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f1b89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f083ae2-0b14-4f52-b473-0de4a8b50664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588201f-ab80-438d-9e5c-de22b7a35f6e",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169beeb2-5632-48f8-87d9-9733ef927a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train data\n",
    "df_train = pd.read_csv('train_home_assignment.csv')\n",
    "print('train shape is: {}'.format(df_train.shape))\n",
    "\n",
    "# Loading the test data\n",
    "df_test = pd.read_csv('test_home_assignment.csv')\n",
    "print('test shape is: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094531b-423f-4a9a-9d72-154be5b497b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First inspect of the train set\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ef4d1-4012-407f-a56d-a68c9344f798",
   "metadata": {},
   "source": [
    "* Seems like the most meaningful features would be indicators for whether a user will take a specific action based on their shopping behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ef3a0-4e97-4ff1-92b2-b6034235c65e",
   "metadata": {},
   "source": [
    "# Train-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee33a4c-8db3-4888-99ca-c5fe9b04f562",
   "metadata": {},
   "source": [
    "* We'd like to first perfrom the train-validation split to ensure that missing values in the validation set are imputed using training set statistics, <br>\n",
    "and to prevent data leakage. <br>\n",
    "We'll use a 80-20 Split (80% for training, 20% for validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552f6c9-bcf2-41da-ba3b-b257d68569e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_train.drop(columns=[\"org_price_usd_following_30_days\"])\n",
    "y = df_train[\"org_price_usd_following_30_days\"]\n",
    "\n",
    "# Perform train-validation split (80-20), ensuring stratified sampling\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Confirm the split sizes\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aaa274-8d96-42f7-8473-bba715347435",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f13989-d460-496f-baf4-9f6677a9ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print some statistics for the train set\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b663e-c61a-4ee0-9aea-71a710a97964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram to show the distribution of the numerical target variable\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=y_train,\n",
    "    nbinsx=20,  # Adjust the number of bins as needed\n",
    "    marker=dict(color='blue', line=dict(width=1)),\n",
    "    opacity=0.75\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Target Variable in Training Set\",\n",
    "    xaxis_title=\"Target Variable\",\n",
    "    yaxis_title=\"Count\",\n",
    "    template=\"plotly_white\",\n",
    "    bargap=0.1\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e5b93-d6d2-427e-b99a-6baeb4c3227e",
   "metadata": {},
   "source": [
    "## Univariate Analysis (Feature Distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa8af7-223a-4769-a62b-40ea0e43fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for distribution analysis\n",
    "numerical_features = [\"#viewed_ads\", \"#times_visited_website\", \"#products_in_cart\", \n",
    "                      \"target_product_price\", \"target_product_description_length\", \"age\"]\n",
    "\n",
    "# Create histograms for numerical features\n",
    "for feature in numerical_features:\n",
    "    fig = px.histogram(X_train, x=feature, nbins=30, title=f\"Distribution of {feature}\",\n",
    "                       template=\"plotly_white\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9501f1-640a-40b7-a73a-c3c014d5f330",
   "metadata": {},
   "source": [
    "Insights from Univariate Analysis: <br>\n",
    "1. Some features (e.g., target_product_price) may have right-skewed distributions, meaning log transformation might help. <br> <br>\n",
    "2. There are some outliers for features like #viewed_ads and #times_visited_website. <br> As the span of the bins is not too large, I do think that these values can be useful for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc309eb5-cc35-4840-9521-9617462d9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical features for distribution analysis\n",
    "categorical_features = [\"target_product_price_color\", \"shopper_segment\", \"delivery_time\", \"target_product_category\"]\n",
    "\n",
    "# Create bar plots for categorical feature distributions with corrected syntax\n",
    "for feature in categorical_features:\n",
    "    category_counts = X_train[feature].value_counts().reset_index()\n",
    "    category_counts.columns = [feature, \"count\"]\n",
    "\n",
    "    fig = px.bar(\n",
    "        category_counts,\n",
    "        x=feature,\n",
    "        y=\"count\",\n",
    "        text=category_counts[\"count\"],\n",
    "        title=f\"Distribution of {feature}\",\n",
    "        labels={feature: feature, \"count\": \"Count\"},\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab078a-40bc-48d5-ba16-09b6da52f12e",
   "metadata": {},
   "source": [
    "<b> Insights from Categorical Feature Analysis: </b> <br>\n",
    "Imbalanced Categories:\n",
    "\n",
    "1. Some categories may be dominant, while others are underrepresented.\n",
    "We may need to group less frequent categories into an \"Other\" category.\n",
    "Feature Encoding Decisions:\n",
    "\n",
    "2. Low cardinality features (shopper_segment, delivery_time) → Label Encoding.\n",
    "High cardinality features (target_product_category, target_product_price_color) → One-Hot Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655c901-8b37-427b-bc84-8360b71a297b",
   "metadata": {},
   "source": [
    "## Bivariate Analysis (Feature Relationships)\n",
    "In this step, we'll analyze how different features relate to the target variable (tag) to uncover important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236755ba-dea0-4e0f-8155-b46d9682ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features\n",
    "numerical_features = [\"#viewed_ads\", \"#times_visited_website\", \"#products_in_cart\", \n",
    "                      \"target_product_price\", \"target_product_description_length\", \"age\"]\n",
    "\n",
    "# Create boxplots for each numerical feature vs. target variable\n",
    "df = pd.concat([X_train, y_train], axis=1)\n",
    "for feature in numerical_features:\n",
    "    fig = px.box(df, x=\"tag\", y=feature, color=\"tag\",\n",
    "                 title=f\"Distribution of {feature} by Target (tag)\",\n",
    "                 labels={\"tag\": \"Tag (0 = No, 1 = Yes)\", feature: feature},\n",
    "                 template=\"plotly_white\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01970cb9-596a-422e-a750-fbb35ec8fafc",
   "metadata": {},
   "source": [
    "<b> Insights from Boxplots </b>\n",
    "\n",
    "Compare distributions for tag=0 vs. tag=1:\n",
    "1. Q: Are users who added more products to the cart (#products_in_cart) more likely to convert? <br>\n",
    "   A: We can see that the median number of products in the cart is higher for tag=1 compared to tag=0.\n",
    "   \n",
    "2. Q: Does age impact conversions? <br>\n",
    "   A: The age distributions for tag=0 and tag=1 are quite similar, meaning age does not significantly impact purchase likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0ab40-b6d2-4ca6-bf76-9179e21f226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features vs. target (tag)\n",
    "categorical_features = [\"target_product_price_color\", \"shopper_segment\", \"delivery_time\"]\n",
    "df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Create bar plots to show the distribution of categorical features for each tag value\n",
    "for feature in categorical_features:\n",
    "    df_counts = df.groupby([feature, \"tag\"]).size().reset_index(name=\"count\")\n",
    "    \n",
    "    # Separate data for tag=0 and tag=1\n",
    "    df_tag_0 = df_counts[df_counts[\"tag\"] == 0]\n",
    "    df_tag_1 = df_counts[df_counts[\"tag\"] == 1]\n",
    "\n",
    "    # Create the figure with **fixed** colors\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add bars for tag=0 (Blue)\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_tag_0[feature], \n",
    "        y=df_tag_0[\"count\"], \n",
    "        name=\"Tag 0 (No)\", \n",
    "        marker_color=\"blue\",\n",
    "        opacity=0.8\n",
    "    ))\n",
    "\n",
    "    # Add bars for tag=1 (Red)\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_tag_1[feature], \n",
    "        y=df_tag_1[\"count\"], \n",
    "        name=\"Tag 1 (Yes)\", \n",
    "        marker_color=\"red\",\n",
    "        opacity=0.8\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Distribution of {feature} by Target (tag)\",\n",
    "        xaxis_title=feature,\n",
    "        yaxis_title=\"Count\",\n",
    "        barmode=\"group\",  # Ensures side-by-side bars\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6ff33-d6ef-4533-a02e-205bc0881678",
   "metadata": {},
   "source": [
    "<b> Insights from Categorical Feature Analysis </b>\n",
    "1. Q: Does shopper_segment affect conversions? <br>\n",
    "   A: Yes! Heavy shoppers likely have a higher probability of conversion (tag=1) than new shoppers\n",
    "   \n",
    "3. Q: Does target_product_price_color affect conversions? <br>\n",
    "   A: Not significantly—There may be minor differences, but no clear trend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c6d86-9b34-42ec-8519-07e3d453ac38",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "Identify highly correlated features (which could be redundant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad693e71-e96e-45fa-bae5-444aa1e9795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for numerical features\n",
    "df = pd.concat([X_train, y_train], axis=1)\n",
    "correlation_matrix = df[[\"#viewed_ads\", \"#times_visited_website\", \"#products_in_cart\", \n",
    "                               \"target_product_price\", \"target_product_description_length\", \"age\", \"tag\"]].corr()\n",
    "\n",
    "# Create a heatmap using Plotly\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=np.round(correlation_matrix.values, 2),\n",
    "    x=list(correlation_matrix.columns),\n",
    "    y=list(correlation_matrix.index),\n",
    "    colorscale=\"blues\",\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title=\"Feature Correlation Heatmap\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Seems like there are no highly correlated numeric features. We might use feature selection techniques afterwards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204c373-fa8c-404e-8754-24c254b59e9f",
   "metadata": {},
   "source": [
    "## Explore missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4e1f8-a38e-4089-b367-1a01ac2ed567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values percentage in the dataset\n",
    "missing_values = X_train.isnull().mean() * 100\n",
    "\n",
    "# Filter only columns with missing values\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "# Create a Plotly bar chart for missing values\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=missing_values.index,\n",
    "        y=missing_values.values,\n",
    "        text=[f\"{p:.2f}%\" for p in missing_values.values],  # Format percentages\n",
    "        textposition='auto',\n",
    "        marker=dict(color='orange'),\n",
    "        opacity=0.8\n",
    "    )\n",
    "])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Percentage of Missing Values per Feature\",\n",
    "    xaxis_title=\"Feature\",\n",
    "    yaxis_title=\"Missing Values (%)\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Seems like there are 3 features with missing values. We'll fill those missing values based on each features' distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2657f7-7be2-4ec1-8af3-209efe36e764",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce9c2f-e4db-400c-b9c1-9d0ba2b85cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remind us of our data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f04505-4889-4c3a-a13a-6d0b992c8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_behavioral_features(df):\n",
    "    df['avg_products_in_cart_per_viewed_ads'] = df['#products_in_cart']/df['#viewed_ads']+1 # Measures intent—high values indicate users add items quickly.\n",
    "    df['avg_products_in_cart_per_times_visited'] = df['#products_in_cart']/df['#times_visited_website']+1   # Measures conversion efficiency\n",
    "    df['avg_viewed_ads_per_times_visited'] = df['#viewed_ads']/df['#times_visited_website']+1   # Measures ad exposure\n",
    "    df['price_per_product'] = df['target_product_price']/df['#products_in_cart']+1  # Captures average price per cart item—budget\n",
    "    # df['avg_products_in_cart_per_viewed_ads'].replace(np.inf,df.loc[df['avg_products_in_cart_per_viewed_ads'] != np.inf, 'avg_products_in_cart_per_viewed_ads'].max(),inplace=True)\n",
    "    # df['avg_products_in_cart_per_times_visited'].replace(np.inf,df.loc[df['avg_products_in_cart_per_times_visited'] != np.inf, 'avg_products_in_cart_per_times_visited'].max(),inplace=True)\n",
    "    # df['avg_viewed_ads_per_times_visited'].replace(np.inf,df.loc[df['avg_viewed_ads_per_times_visited'] != np.inf, 'avg_viewed_ads_per_times_visited'].max(),inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadf3d8-83e8-4c5e-8158-95716523a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorical_features_encoding(df):\n",
    "    \"\"\"\n",
    "    Transforms categorical features in the given dataframe by:\n",
    "    1. Applying Label Encoding to low-cardinality categorical features.\n",
    "    2. Splitting 'target_product_category' into primary and secondary categories.\n",
    "    3. Applying One-Hot Encoding to 'primary_target_product_category' and 'secondary_target_product_category'.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe containing categorical features.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Transformed dataframe with encoded categorical features.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Copy dataframe to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Label Encoding for low-cardinality categorical features\n",
    "    label_encode_features = [\"shopper_segment\", \"delivery_time\"]\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for feature in label_encode_features:\n",
    "        le = LabelEncoder()\n",
    "        df[feature] = le.fit_transform(df[feature])\n",
    "        label_encoders[feature] = le  # Store encoders for reference\n",
    "\n",
    "    # Splitting 'target_product_category' into primary and secondary categories\n",
    "    df[[\"primary_target_product_category\", \"secondary_target_product_category\"]] = df[\n",
    "        \"target_product_category\"\n",
    "    ].str.split(\" - \", n=1, expand=True)\n",
    "\n",
    "    # Filling missing values in 'secondary_target_product_category' (if no secondary exists, fill with \"None\")\n",
    "    df[\"secondary_target_product_category\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "    # One-Hot Encoding for primary and secondary categories\n",
    "    df = pd.get_dummies(df, columns=[\"primary_target_product_category\", \"secondary_target_product_category\"], drop_first=True)\n",
    "\n",
    "    # One-Hot Encoding for 'target_product_price_color'\n",
    "    df = pd.get_dummies(df, columns=[\"target_product_price_color\"], drop_first=True)\n",
    "\n",
    "    # Drop the original 'target_product_category' column\n",
    "    df.drop(columns=[\"target_product_category\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f02e4b-87b4-41fc-b147-a21863f4f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sin_cos_transformation(df):\n",
    "    \"\"\"\n",
    "    Applies Sin/Cos transformation to the 'timestamp' feature by extracting the hour of the day\n",
    "    and encoding it as two cyclic features: sin(hour) and cos(hour).\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe containing the 'timestamp' column.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Transformed dataframe with 'sin_hour' and 'cos_hour' features.\n",
    "    \"\"\"\n",
    "    # Copy dataframe to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert 'timestamp' to datetime format and extract the hour\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%H:%M:%S\")\n",
    "    df[\"hour_of_day\"] = df[\"timestamp\"].dt.hour  # Extract hour (0-23)\n",
    "\n",
    "    # Apply sin/cos transformation\n",
    "    df[\"sin_hour\"] = np.sin(2 * np.pi * df[\"hour_of_day\"] / 24)\n",
    "    df[\"cos_hour\"] = np.cos(2 * np.pi * df[\"hour_of_day\"] / 24)\n",
    "\n",
    "    # Drop the original timestamp column and hour column\n",
    "    df.drop(columns=[\"timestamp\", \"hour_of_day\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Bucketing (morning: 6-12, afternoon: 12-18) creates hard boundaries that may not reflect actual user behavior.\")\n",
    "print(\"By using sin(hour) and cos(hour), the transition between hours is smooth and distance-based models (e.g. XGBoost) can interpret time more effectively.\")\n",
    "print(\"This preserves the cyclic nature of time, making 23:00 and 01:00 close in value rather than distant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d9883-898b-4c50-a184-a7aebdffc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Imputes missing values for numerical features using appropriate strategies:\n",
    "    - Uses median imputation for `#viewed_ads`, `age` and `#times_visited_website`.\n",
    "    - Uses  median imputation for `age` based on `shopper_segment`.\n",
    "    \n",
    "    Median imputation is robust against outliers and skewed distributions.\n",
    "    median for age helps preserve user demographics.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with missing values.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Transformed dataframe with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Median imputation for numerical features\n",
    "    num_features = [\"#viewed_ads\", \"#times_visited_website\", \"age\"]\n",
    "    for feature in num_features:\n",
    "        df[feature].fillna(df[feature].median(), inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1590f87-2649-4689-b4a8-3c541fdca2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sklearn transformer\n",
    "class CustomDataTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom sklearn Transformer that applies:\n",
    "    1. Missing value imputation\n",
    "    2. Categorical feature encoding (Label Encoding & One-Hot Encoding)\n",
    "    3. Timestamp transformation (Sin/Cos encoding for cyclic time representation)\n",
    "    4. Behavioral feature engineering (Ratio-based features)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer (for label encoding categories).\n",
    "        \"\"\"\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply all transformations to the dataset.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "\n",
    "        # Apply missing value imputation\n",
    "        X = impute_missing_values(X)\n",
    "\n",
    "        # Apply categorical feature encoding\n",
    "        X = create_categorical_features_encoding(X)\n",
    "\n",
    "        # Apply timestamp transformation (sin/cos encoding)\n",
    "        X = apply_sin_cos_transformation(X)\n",
    "\n",
    "        # Create behavioral features\n",
    "        #X = create_behavioral_features(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7fcfd7-0c40-469c-9d50-585c2ee84e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the transformer on training data only\n",
    "transformer = CustomDataTransformer()\n",
    "X_train_transformed = transformer.fit_transform(X_train)  # Fit and transform on training set\n",
    "\n",
    "# Apply the same transformations to validation data (using fitted transformer)\n",
    "X_val_transformed = transformer.transform(X_val)  # Transform validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ef910-9554-4da9-9242-af14513b2455",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af998fc5-1767-4d6c-ba11-426299319618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_prob, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Generates an interactive ROC Curve using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True binary labels.\n",
    "    y_prob (array-like): Predicted probabilities for the positive class.\n",
    "    model_name (str): Name of the model (for labeling).\n",
    "    \"\"\"\n",
    "    # Compute ROC curve points\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add ROC Curve\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f\"{model_name} ROC Curve\", line=dict(color='blue')))\n",
    "    \n",
    "    # Add Diagonal Reference Line (Random Model)\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random Guess', line=dict(dash='dash', color='black')))\n",
    "\n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        title=f\"ROC Curve for {model_name}\",\n",
    "        xaxis_title=\"False Positive Rate (FPR)\",\n",
    "        yaxis_title=\"True Positive Rate (Recall)\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0d376-fc8f-4ce2-a621-58c799e1ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Generates an interactive confusion matrix heatmap using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True binary labels.\n",
    "    y_pred (array-like): Predicted labels.\n",
    "    model_name (str): Name of the model (for labeling).\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Define labels\n",
    "    labels = [\"True Negative\", \"False Positive\", \"False Negative\", \"True Positive\"]\n",
    "\n",
    "    # Create the annotation matrix\n",
    "    annotations = [[f'{value}' for value in row] for row in cm]\n",
    "\n",
    "    # Generate heatmap using Plotly\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=cm,\n",
    "        x=['Pred: 0', 'Pred: 1'],\n",
    "        y=['Actual: 0', 'Actual: 1'],\n",
    "        colorscale=\"Blues\",\n",
    "        annotation_text=annotations,\n",
    "        showscale=False\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Confusion Matrix for {model_name}\",\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"True Label\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5775d-79df-45c7-8ada-74b3d889d6f6",
   "metadata": {},
   "source": [
    "## Create a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c1f1b-cc1b-4332-b2ef-7064a001e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Decision Tree model with a reasonable depth to prevent overfitting\n",
    "baseline_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = baseline_model.predict(X_val_transformed)\n",
    "y_prob = baseline_model.predict_proba(X_val_transformed)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Evaluate model performance\n",
    "baseline_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "    \"Precision\": precision_score(y_val, y_pred),\n",
    "    \"Recall\": recall_score(y_val, y_pred),\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_prob)\n",
    "}\n",
    "\n",
    "baseline_results = pd.DataFrame([baseline_metrics])\n",
    "print(\"Baseline Model Performance:\\n\", baseline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a16b39-67d9-43b4-b156-fcc24eb84833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the ROC curve for the baseline Decision Tree model\n",
    "plot_roc_curve(y_val, y_prob, model_name=\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477af7e-d53f-4b8e-b3bb-16503b49e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Confusion Matrix for the Decision Tree baseline model\n",
    "plot_confusion_matrix(y_val, y_pred, model_name=\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a4a2e-0ebf-47e3-8557-fc7b6781dc1f",
   "metadata": {},
   "source": [
    "## Find best parameters for each advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873008bf-fef3-4371-a8ff-9f25064bba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final hyperparameter tuning & evaluation function\n",
    "def tune_and_evaluate_model(model_name, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Tunes hyperparameters using GridSearchCV, trains the best model, evaluates its performance,\n",
    "    and plots the ROC curve and confusion matrix using existing functions.\n",
    "\n",
    "    Parameters:\n",
    "    model_name (str): \"random_forest\" or \"xgboost\"\n",
    "    X_train (pd.DataFrame): Transformed training features.\n",
    "    y_train (pd.Series): Training labels.\n",
    "    X_val (pd.DataFrame): Transformed validation features.\n",
    "    y_val (pd.Series): Validation labels.\n",
    "\n",
    "    Returns:\n",
    "    best_model: Trained model with best parameters.\n",
    "    best_params (dict): Best hyperparameters found by GridSearchCV.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter grids\n",
    "    param_grids = {\n",
    "        \"random_forest\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [5, 10],\n",
    "            \"min_samples_split\": [2, 5]\n",
    "        },\n",
    "        \"xgboost\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [3, 6],\n",
    "            \"learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Model selection\n",
    "    if model_name == \"random_forest\":\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    elif model_name == \"xgboost\":\n",
    "        model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose 'random_forest' or 'xgboost'.\")\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grids[model_name],\n",
    "        cv=5,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve best model and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Make predictions on validation set\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    auc_score = roc_auc_score(y_val, y_prob)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(f\"Best {model_name.upper()} Parameters: {best_params}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"ROC-AUC: {auc_score:.4f}\")\n",
    "\n",
    "    # Plot ROC Curve using existing function\n",
    "    plot_roc_curve(y_val, y_prob, model_name=model_name.upper())\n",
    "\n",
    "    # Plot Confusion Matrix using existing function\n",
    "    plot_confusion_matrix(y_val, y_pred, model_name=model_name.upper())\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "# Tune, train, and evaluate Random Forest\n",
    "best_rf_model, best_rf_params = tune_and_evaluate_model(\"random_forest\", X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "# Tune, train, and evaluate XGBoost\n",
    "best_xgb_model, best_xgb_params = tune_and_evaluate_model(\"xgboost\", X_train_transformed, y_train, X_val_transformed, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deee99-5911-4c51-8e16-08e9a0a182a5",
   "metadata": {},
   "source": [
    "# Final prediction\n",
    "We can see that the XGBoost model is the best model. We'll choose it to generate our final predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90299594-4153-4cb6-aea7-dd01063e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing steps\n",
    "X_test_transformed = transformer.transform(df_test)  # Use the trained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cd901-d2ad-42fe-9dd7-3e5b68b087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = X_test_transformed.drop(columns=[\"id\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f64bb0-2017-4235-b827-a4842e6e4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the test set\n",
    "y_test_prob = best_xgb_model.predict_proba(X_test_transformed)[:, 1]  # Get probability for class 1\n",
    "\n",
    "# Predict class labels\n",
    "y_test_pred = best_xgb_model.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb871d4-7553-406b-aa38-93f0f9a4f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for submission\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"id\": test_df.index,  # Adjust this based on the test dataset structure\n",
    "    \"predicted_prob\": y_test_prob,  # Probability of class 1\n",
    "    \"predicted_class\": y_test_pred   # Predicted class label\n",
    "})\n",
    "\n",
    "# Save to CSV file\n",
    "test_predictions.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924056c-d016-46b1-8275-aaa9271d2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics for the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "auc_test = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f\"Test Set Performance for Random Forest:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"ROC-AUC: {auc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3ad19-a168-492a-aa64-d167d826841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve using existing function\n",
    "plot_roc_curve(y_test, y_test_prob, model_name=\"Random Forest (Test Set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1bdfb-a4de-4647-b9be-664b0b350aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix using existing function\n",
    "plot_confusion_matrix(y_test, y_test_pred, model_name=\"Random Forest (Test Set)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
