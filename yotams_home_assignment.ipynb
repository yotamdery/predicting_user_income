{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51baa487-f643-45aa-a09a-51bfb6a4eca7",
   "metadata": {},
   "source": [
    "# User income prediction\n",
    "## Author: Yotam Dery\n",
    "## Date: 03/03/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd0c43-e464-4c05-a28d-3945237808e7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f083ae2-0b14-4f52-b473-0de4a8b50664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "# Viz related\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "# ML related\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from scipy.stats import skew\n",
    "# Projects scripts\n",
    "from prediction_task_plot_utils import plot_target_label, plot_features_dist, plot_corr_with_label\n",
    "from prediction_task_utils import check_skewness_score, apply_log_transformation, \\\n",
    "                                    identify_skewed_features, identify_highly_correlated_features,\\\n",
    "                                    get_outliers_top_n_features\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588201f-ab80-438d-9e5c-de22b7a35f6e",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169beeb2-5632-48f8-87d9-9733ef927a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train data\n",
    "df_train = pd.read_csv('train_home_assignment.csv', index_col=0)\n",
    "# Dropping columns for this task\n",
    "df_train = df_train.drop(columns=[\"treatment\", \"org_price_usd_following_30_days_after_impact\"])\n",
    "print('train shape is: {}'.format(df_train.shape))\n",
    "\n",
    "# Loading the test data\n",
    "df_test = pd.read_csv('test_home_assignment.csv', index_col=0)\n",
    "print('test shape is: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094531b-423f-4a9a-9d72-154be5b497b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First inspect of the train set\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ef3a0-4e97-4ff1-92b2-b6034235c65e",
   "metadata": {},
   "source": [
    "# Train-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee33a4c-8db3-4888-99ca-c5fe9b04f562",
   "metadata": {},
   "source": [
    "* We'd like to first perfrom the train-validation split to ensure that missing values in the validation set are imputed using training set statistics, <br>\n",
    "and to prevent data leakage. <br>\n",
    "We'll use a 80-20 Split (80% for training, 20% for validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552f6c9-bcf2-41da-ba3b-b257d68569e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_train.drop(columns=[\"org_price_usd_following_30_days\"])\n",
    "y = df_train[\"org_price_usd_following_30_days\"]\n",
    "\n",
    "# Create separate targets:\n",
    "y_log = np.log1p(y)  # Log transformation (only for Linear Regression)\n",
    "y_original = y  # Keep original target for tree-based models\n",
    "\n",
    "# Split into training & validation sets\n",
    "X_train, X_val, y_train_log, y_val_log = train_test_split(X, y_log, test_size=0.2, random_state=42)  # Log target for linear\n",
    "_, _, y_train_original, y_val_original = train_test_split(X, y_original, test_size=0.2, random_state=42)  # Original target for trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aaa274-8d96-42f7-8473-bba715347435",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f13989-d460-496f-baf4-9f6677a9ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print some statistics for the train set\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic structure of the dataset\n",
    "train_info = X_train.info()\n",
    "train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_info = X_val.info()\n",
    "validation_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539be0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = X_train.isnull().sum().sort_values(ascending=False)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae46a13",
   "metadata": {},
   "source": [
    "* <b>Insights from previous steps:</b>\n",
    "1. Training set: 160,000 rows\n",
    "Validation set: 40,000 rows \n",
    "\n",
    "2. Dataset Structure\n",
    "53 features (excluding the target variable). <br>\n",
    "All features are numerical (float64 or int64). <br>\n",
    "There are no missing values. <br>\n",
    "Some features have high variance, indicating possible outliers. <br>\n",
    "Certain features have long tails, meaning a log transformation may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b663e-c61a-4ee0-9aea-71a710a97964",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the target label histogram\n",
    "# Define fixed bin edges from 0 to 800 with intervals of 50\n",
    "start, end, step = 0, 850, 50\n",
    "bin_edges = np.arange(start, end, step)  # 850 ensures last bin covers up to 800\n",
    "plot_target_label(bin_edges, y_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c722a",
   "metadata": {},
   "source": [
    "* The target label looks very skewed (strong right tail). <br> If we consider models that assume normally distributed residuals (like linear regression), we might want to perform a log transformation to the target label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0806a2",
   "metadata": {},
   "source": [
    "* <b> Skewness criteria: </b> <br>\n",
    "Skewness > 1.0 → highly skewed (log transform might help). <br>\n",
    "Skewness between 0.5 and 1.0 → moderately skewed (consider transforming). <br>\n",
    "Skewness < 0.5 → nearly symmetric (transformation unnecessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28509132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_skewness_score(y_train_original))\n",
    "print(\"Looks like the target label is highly skewed before the trasformation and symmetric after!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98aa57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transformed target label\n",
    "y_train_log_transformed = apply_log_transformation(y_train_original)\n",
    "start, end, step = 0, 11, 1\n",
    "bin_edges = np.arange(start, end, step)  \n",
    "plot_target_label(bin_edges, y_train_log_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e5b93-d6d2-427e-b99a-6baeb4c3227e",
   "metadata": {},
   "source": [
    "## Univariate Analysis (Features Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa8af7-223a-4769-a62b-40ea0e43fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_dist(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d49420",
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_skewed_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For a deeper profiling (but takes long time for execution)\n",
    "# profile = ProfileReport(X_train, explorative=True)\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9501f1-640a-40b7-a73a-c3c014d5f330",
   "metadata": {},
   "source": [
    "Insights from Univariate Analysis: <br>\n",
    "1. Many real-world use cases (like user spending, income, and engagement metrics) follow a right-skewed distribution (like we see here!).<br> <br>\n",
    "2. Some features (e.g., payment_occurrences_preceding_30_days,payment_occurrences_preceding_3_days) have right-skewed distributions, meaning log transformation might help. <br> <br>\n",
    "3. There are some outliers for features like #viewed_ads and #times_visited_website. <br> As the span of the bins is not too large, I do think that these values can be useful for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655c901-8b37-427b-bc84-8360b71a297b",
   "metadata": {},
   "source": [
    "## Bivariate Analysis (Feature Relationships)\n",
    "In this step, we'll analyze how different features relate to the target variable (tag) to uncover important patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236755ba-dea0-4e0f-8155-b46d9682ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_with_label(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for highly correlated pairs of features\n",
    "identify_highly_correlated_features(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab319924",
   "metadata": {},
   "source": [
    "Here are all pairs of highly correlated features (corr>0.9).<br> Some highly correlated features might capture different patterns (e.g., spending behavior vs. engagement behavior), <br>so we wont drop them for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7752cb0",
   "metadata": {},
   "source": [
    "## Outliers Detection\n",
    "* Here we perform the outliers detection for the top 20 highly correlated features (with respect to the target label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0ab40-b6d2-4ca6-bf76-9179e21f226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_outliers_top_n_features(X_train, y_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4e1f8-a38e-4089-b367-1a01ac2ed567",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1590f87-2649-4689-b4a8-3c541fdca2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer to preprocess data for machine learning models.\n",
    "    \n",
    "    - For Linear Regression:\n",
    "        - Applies log transformation (with automatic negative value shifting).\n",
    "        - Removes outliers using IQR clipping.\n",
    "        - Standardizes features using StandardScaler.\n",
    "        - Optionally removes highly correlated features.\n",
    "    \n",
    "    - For Tree-Based Models (Random Forest, XGBoost):\n",
    "        - Skips outlier removal (trees handle outliers naturally).\n",
    "        - Skips scaling (trees do not require feature scaling).\n",
    "        - Optionally removes highly correlated features.\n",
    "\n",
    "    Parameters:\n",
    "        model_type (str): \"linear\" for Linear Regression, \"tree\" for RF/XGBoost.\n",
    "        remove_high_corr_features (bool): If True, removes highly correlated features.\n",
    "\n",
    "    Methods:\n",
    "        fit(X, y): Learns dataset statistics.\n",
    "        transform(X): Applies transformations using learned statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_type=\"linear\", remove_high_corr_features=True):\n",
    "        self.model_type = model_type\n",
    "        self.remove_high_corr_features = remove_high_corr_features\n",
    "        self.log_features = []\n",
    "        self.scaler = None\n",
    "        self.iqr_limits = {}\n",
    "        self.high_corr_features = {}\n",
    "        self.shift_values = {}  # Stores shift values for negative log-transformed features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learns dataset statistics for transformations.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Training data.\n",
    "            y (pd.Series, optional): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        # Identify skewed features for log transformation (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            skewness = X.apply(lambda x: skew(x), axis=0)\n",
    "            self.log_features = skewness[abs(skewness) > 1].index.tolist()\n",
    "\n",
    "            # Detect and store shift values for negative log-transformed features\n",
    "            for feature in self.log_features:\n",
    "                min_value = X[feature].min()\n",
    "                if min_value <= 0:\n",
    "                    self.shift_values[feature] = abs(min_value) + 1  # Shift to make values positive\n",
    "\n",
    "        # Compute IQR thresholds for outlier handling (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            for feature in X.columns:\n",
    "                Q1 = X[feature].quantile(0.25)\n",
    "                Q3 = X[feature].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                self.iqr_limits[feature] = (lower_bound, upper_bound)\n",
    "\n",
    "        # Compute feature scaling using StandardScaler (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(X)\n",
    "\n",
    "        # Identify highly correlated features for removal (if enabled)\n",
    "        if self.remove_high_corr_features:\n",
    "            corr_matrix = X.corr()\n",
    "            high_corr_pairs = set()\n",
    "            for col in corr_matrix.columns:\n",
    "                for idx in corr_matrix.index:\n",
    "                    if col != idx and abs(corr_matrix.loc[idx, col]) > 0.9:\n",
    "                        high_corr_pairs.add((col, idx))\n",
    "            self.high_corr_features = list(set([pair[1] for pair in high_corr_pairs]))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies transformations to the dataset using the learned statistics.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Dataset to transform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed dataset.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Apply log transformation (only for Linear Regression)\n",
    "        if self.model_type == \"linear\" and self.log_features:\n",
    "            for feature in self.log_features:\n",
    "                # Apply the shift if required (to prevent log of negative numbers)\n",
    "                if feature in self.shift_values:\n",
    "                    X_transformed[feature] += self.shift_values[feature]\n",
    "                X_transformed[feature] = np.log1p(X_transformed[feature])\n",
    "\n",
    "        # Apply outlier handling using IQR clipping (only for Linear Regression)\n",
    "        if self.model_type == \"linear\":\n",
    "            for feature, (lower_bound, upper_bound) in self.iqr_limits.items():\n",
    "                X_transformed[feature] = np.clip(X_transformed[feature], lower_bound, upper_bound)\n",
    "\n",
    "        # Apply feature scaling using StandardScaler (only for Linear Regression)\n",
    "        if self.model_type == \"linear\" and self.scaler:\n",
    "            X_transformed = pd.DataFrame(self.scaler.transform(X_transformed), columns=X_transformed.columns)\n",
    "\n",
    "        # Drop highly correlated features (if enabled)\n",
    "        if self.remove_high_corr_features and self.high_corr_features:\n",
    "            X_transformed = X_transformed.drop(columns=self.high_corr_features, errors=\"ignore\")\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7fcfd7-0c40-469c-9d50-585c2ee84e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transformer for Linear Regression (without correlated feature removal)\n",
    "preprocessor_linear = CustomPreprocessor(model_type=\"linear\", remove_high_corr_features=False)\n",
    "\n",
    "# Fit and transform for Linear Regression\n",
    "preprocessor_linear.fit(X_train)\n",
    "X_train_transformed_linear = preprocessor_linear.transform(X_train)\n",
    "X_val_transformed_linear = preprocessor_linear.transform(X_val)\n",
    "# Output transformed training data sample (Linear Model)\n",
    "X_train_transformed_linear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a764d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transformer for Tree-Based Models (without correlated feature removal)\n",
    "preprocessor_tree = CustomPreprocessor(model_type=\"tree\", remove_high_corr_features=False)\n",
    "\n",
    "# Fit and transform for Tree-Based Models\n",
    "preprocessor_tree.fit(X_train)\n",
    "X_train_transformed_tree = preprocessor_tree.transform(X_train)\n",
    "X_val_transformed_tree = preprocessor_tree.transform(X_val)\n",
    "\n",
    "# Output transformed training data sample (Tree-Based Model)\n",
    "X_train_transformed_tree.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ef910-9554-4da9-9242-af14513b2455",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5775d-79df-45c7-8ada-74b3d889d6f6",
   "metadata": {},
   "source": [
    "## Create a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model using the preprocessed training data (X_train) and log-transformed target (y_train_log)\n",
    "linear_model.fit(X_train_transformed_linear, y_train_log)\n",
    "\n",
    "# Predict on the validation set (output is still in log scale)\n",
    "y_pred_log = linear_model.predict(X_val_transformed_linear)\n",
    "\n",
    "# Convert predictions back to original scale using expm1()\n",
    "y_pred_original = np.expm1(y_pred_log)  # Reverse log1p transformation\n",
    "\n",
    "# Compute RMSE (Root Mean Squared Error) in the original scale\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_val_original, y_pred_original))\n",
    "\n",
    "# Output RMSE\n",
    "rmse_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9cead",
   "metadata": {},
   "source": [
    "## Create and tune the tree based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c1f1b-cc1b-4332-b2ef-7064a001e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200],  # Number of trees\n",
    "    \"max_depth\": [10, 20],  # Depth of trees\n",
    "    \"min_samples_split\": [5, 10],  # Minimum samples to split a node\n",
    "    \"min_samples_leaf\": [2, 5],  # Minimum samples per leaf\n",
    "}\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42, criterion=\"squared_error\")\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_model, rf_param_grid, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, verbose=1\n",
    ")\n",
    "rf_grid_search.fit(X_train_transformed_tree, y_train_original)\n",
    "\n",
    "# Get best model & best parameters\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "best_rf_params = rf_grid_search.best_params_\n",
    "\n",
    "# Predict using best model\n",
    "y_pred_rf_best = best_rf_model.predict(X_val_transformed_tree)\n",
    "\n",
    "# Compute RMSE for best RF model\n",
    "rmse_rf_best = np.sqrt(mean_squared_error(y_val_original, y_pred_rf_best))\n",
    "\n",
    "# Output best parameters and RMSE\n",
    "best_rf_params, rmse_rf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200],  # Number of boosting rounds\n",
    "    \"max_depth\": [4, 6],  # Depth of trees\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],  # Step size for each tree\n",
    "    \"subsample\": [0.8, 1.0],  # % of data used in each boosting round\n",
    "    \"colsample_bytree\": [0.8, 1.0],  # % of features used in each boosting round\n",
    "}\n",
    "\n",
    "# Initialize XGBoost\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    xgb_model, xgb_param_grid, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, verbose=1\n",
    ")\n",
    "xgb_grid_search.fit(X_train_transformed_tree, y_train_original)\n",
    "\n",
    "# Get best model & best parameters\n",
    "best_xgb_model = xgb_grid_search.best_estimator_\n",
    "best_xgb_params = xgb_grid_search.best_params_\n",
    "\n",
    "# Predict using best XGBoost model\n",
    "y_pred_xgb_best = best_xgb_model.predict(X_val_transformed_tree)\n",
    "\n",
    "# Compute RMSE for best XGBoost model\n",
    "rmse_xgb_best = np.sqrt(mean_squared_error(y_val_original, y_pred_xgb_best))\n",
    "\n",
    "# Output best parameters and RMSE\n",
    "best_xgb_params, rmse_xgb_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e37048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70deee99-5911-4c51-8e16-08e9a0a182a5",
   "metadata": {},
   "source": [
    "# Final prediction\n",
    "We can see that the XGBoost model is the best model. We'll choose it to generate our final predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90299594-4153-4cb6-aea7-dd01063e66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing steps\n",
    "X_test_transformed = transformer.transform(df_test)  # Use the trained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cd901-d2ad-42fe-9dd7-3e5b68b087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = X_test_transformed.drop(columns=[\"id\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f64bb0-2017-4235-b827-a4842e6e4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the test set\n",
    "y_test_prob = best_xgb_model.predict_proba(X_test_transformed)[:, 1]  # Get probability for class 1\n",
    "\n",
    "# Predict class labels\n",
    "y_test_pred = best_xgb_model.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb871d4-7553-406b-aa38-93f0f9a4f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for submission\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"id\": test_df.index,  # Adjust this based on the test dataset structure\n",
    "    \"predicted_prob\": y_test_prob,  # Probability of class 1\n",
    "    \"predicted_class\": y_test_pred   # Predicted class label\n",
    "})\n",
    "\n",
    "# Save to CSV file\n",
    "test_predictions.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924056c-d016-46b1-8275-aaa9271d2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics for the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "auc_test = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f\"Test Set Performance for Random Forest:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"ROC-AUC: {auc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3ad19-a168-492a-aa64-d167d826841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve using existing function\n",
    "plot_roc_curve(y_test, y_test_prob, model_name=\"Random Forest (Test Set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1bdfb-a4de-4647-b9be-664b0b350aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix using existing function\n",
    "plot_confusion_matrix(y_test, y_test_pred, model_name=\"Random Forest (Test Set)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
